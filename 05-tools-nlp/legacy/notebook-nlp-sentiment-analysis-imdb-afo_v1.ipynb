{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Data-X Notebook: NLP & NLTK\n",
    "#### Using Natural Language Processing in Python to do sentiment analysis on IMDB movie reviews\n",
    "\n",
    "Author: Alexander Fred Ojala\n",
    "\n",
    "Source: \n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "- https://github.com/rasbt/python-machine-learning-book/tree/master/code\n",
    "- https://github.com/justmarkham/pycon-2016-tutorial\n",
    "\n",
    "*Code snippets given run on both Python 2.7 and Python 3.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "### - [Part 0: Pre-Setup & Data description](#sec0)\n",
    "### - [Part 1: Explore the Data](#sec1)\n",
    "### - [Part 2: NLTK intro (1 review) cleaning, tokenizing, stemming, lemmatization, stopwords](#sec2)\n",
    "### - [Part 3: Preparing the data set for classification](#sec3)\n",
    "### - [Part 4: Sentiment Classification w scikit-learn, Feature vectors & Bag of Words model](#sec4)\n",
    "#### - [Appendix: Submitting to Kaggle & possible extensions](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec0'></div>\n",
    "## Part 0: Pre-Setup & Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "\n",
    "You can download the data (labeledTrainData.tsv.zip) here: https://www.kaggle.com/c/word2vec-nlp-tutorial/data, place it in your working directory & unzip the file. (It is placed in the data folder if you cloned the Github repo)\n",
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n4 data/labeledTrainData.tsv \n",
    "# use bash command to see data set structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data to a Pandas data frame\n",
    "# Use header = 0 (first line contains col names)\n",
    "# use delimiter=\\t (columns are separated by tabs),\n",
    "# use quoting=3 (Python will ignore doubled quotes)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info() # no NaN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: How many movie reviews are positive and how many are negative in labeledTrainData.tsv? Do we have balance between the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "print(train.shape)\n",
    "print()\n",
    "print('Number of Data Samples for every label output. 1=postive, 2=negative:')\n",
    "print(train.sentiment.value_counts())\n",
    "\n",
    "train.sentiment.hist(); #class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: What is the average length of all the reviews (string length)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply length function to the review column\n",
    "lengths = train.review.apply(len)\n",
    "\n",
    "print('Average character length of the reviews are:')\n",
    "print (np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec2'></div>\n",
    "# Part 2: \n",
    "## NLTK intro (1 review) cleaning, tokenizing, stemming, lemmatization, stopwords\n",
    "\n",
    "Let's explore NLP by looking at the third review in the training data set, i.e. `train['review'][2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review3 = train['review'][2] # the review used for initial analysis\n",
    "print(review3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the review\n",
    "\n",
    "First we would like to clean up the reviews. As you can see many interviews contain \\ characters in front of quotation symobols, \"`<br/>` tags, numbers, abbrevations etc.\n",
    "\n",
    "### 1: Remove all the HTML tags in the third review, by creating a beatifulsoup object and then using the `.text` method. Save results in variable `review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "review3 = bs.BeautifulSoup(review3,features='lxml').text # removes HTML tags\n",
    "print(review3[:1150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Import NLTK's sent_tokenizer and count the number of sentences in review 3 \n",
    "The review should be cleaned from HTML tags. To import sent_tokenizer use: `from nltk.tokenize import sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sent_tokenize(review3)))\n",
    "sent_tokenize(review3) # doesn't really split all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it does a better job if we add space after every period\n",
    "review3 = review3.replace('.','. ')\n",
    "\n",
    "print(len(sent_tokenize(review3)), end='\\n\\n') # number of sentences\n",
    "\n",
    "# print all sentences on a new line\n",
    "for sent in sent_tokenize(review3):\n",
    "    print(sent, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Remove all punctuation and special characters from the third review.\n",
    "We can do this using Regular Expression - package `re`. Regular expression define search patterns for text that can be used to search, replace, substitute etc. certain patterns in text data.\n",
    "\n",
    "We'll use regular expression to only look for alphabetical characters, `[^a-zA-Z]`. Save results in variable `review3`\n",
    "\n",
    "Intro to Regex: http://www.aivosto.com/vbtips/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review3 = re.sub('[^a-zA-Z]',' ',review3)\n",
    "print(review3) # remove special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Convert all the letters to lower case, split the string so that every word is one element in a list\n",
    "Note: When we split the strings into words that process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review3 = review3.lower()\n",
    "review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review3_words = review3.split()\n",
    "print(review3_words[:10]) # tokenize and lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Stemming\n",
    "Use NLTK's PorterStemmer (`from nltk.stem import PorterStemmer`). Create a new Porter stemmer (`stemmer = PorterStemmer()`) and run it on every word in `review3_words`, print the results as one string (don't overwrite the `review3_words` variable from 4). \n",
    "\n",
    "**What does the PorterStemmer do?**\n",
    "PorterStemmer tries treating similar words as the same (e.g. give, gives, given = give).\n",
    "\n",
    "**Stemming (paraphrased from Wiki):** Stemming reduces words to their word stem, base or root form (i.e. similar words, with the same stem, will be interpreted as the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps = PorterStemmer() #initialize Porter Stemmer object\n",
    "\n",
    "ps_stems = []\n",
    "for w in review3_words:\n",
    "    ps_stems.append(ps.stem(w))\n",
    "\n",
    "print(' '.join(ps_stems)) # add all the stemmed words to one string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Part of Speech tagging\n",
    "Now we want to Part Of Speech (POS)) tag the third movie review. We will use POS labeling, also called grammatical tagging. To do this import `from nltk.tag import pos_tag`. \n",
    "\n",
    "When you use `pos_tag` on a word it returns a token-tag pair in the form of a tuple. In NLTK's Penn Treebank POS, the abbreviation (tag) for an adjective is JJ and NN for singular nouns. \n",
    "\n",
    "Let's count the number of singular nouns (NN) and adjectives (JJ) in `review3_words` using NLTK's pos_tag. A list of the Penn Treebank pos_tag's can be found here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "token_tag = pos_tag(review3_words)\n",
    "token_tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell that count nouns (NN) and adjectives (JJ) in review3\n",
    "NN_count = 0\n",
    "JJ_count = 0\n",
    "\n",
    "for pair in token_tag:\n",
    "    tag = pair[1]\n",
    "    if tag == 'JJ':\n",
    "        JJ_count+=1\n",
    "    elif tag == 'NN':\n",
    "        NN_count+=1\n",
    "print('Number of nouns:', NN_count)\n",
    "print('Number of adjectives:', JJ_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lemmatizing\n",
    "An even more sophisticated operation than stemming using the PorterStemmer is called lemmatizing. \n",
    "\n",
    "Lemmatizing, in contrast to stemming, does not create non-existent words and converts words to their synonyms. In order to use lemmatizing we need to define the wordnet POS tag. A function that takes in a POS Penn Treebank tag and converts it to a wordnet tag and then lemmatizes words in a string has been given written below. \n",
    "\n",
    "**Lemmatization (paraphrased from Wiki):** Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "We can use this to print the lemmatized third movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. StopWords\n",
    "\n",
    "Lastly we will remove common words that don't carry much information. These are called stopwords. \n",
    "\n",
    "In English they could for example be 'am', 'are', 'and' etc. \n",
    "\n",
    "To import NLTK's list of stopwords you need to download the stopword corpora (`import nltk` and then `nltk.download()` if you don't have it). \n",
    "\n",
    "When that is done run `from nltk.corpus import stopwords` and create a variable for English stopwords with `eng_stopwords = stopwords.words('english')`. Use the list of English stopwords to remove all the stopwords from your list of words in the third movie review, i.e. `review3_words`. Print `review3_words` without stopwords, count the number of stopwords removed and print them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review3_wo_stopwords = [w for w in review3_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in review3_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print('REVIEW WITHOUT STOPWORDS:')\n",
    "print(' '.join(review3_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec3'></div>\n",
    "# Part 3: Preparing the data set for classification\n",
    "\n",
    "In this part we'll put everything that we've learned so far together. This so that we can use our reviews to make accurate predicitons on their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def review_cleaner(review):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review = ' '.join(review+emoticons)\n",
    "    # add emoticons to the end\n",
    "\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create original, Porter Stemmed, and Lemmatized data sets\n",
    "    \n",
    "**Create three lists `review_clean_original`, `review_clean_ps` and `review_clean_wnl`.**\n",
    "\n",
    "- `review_clean_original` contains all the reviews from the train DataFrame, that have been cleaned by the function `review_cleaner` defined above.\n",
    "\n",
    "- `review_clean_ps` applies the PorterStemmer to the reviews in `review_clean_original`. **Note:** NLTK version 3.2.2 crashes when trying to use the PorterStemming on the string 'oed' (known bug). Therefore, use an if statement to skip just that specific string/word.\n",
    "\n",
    "- `review_clean_wnl` contains words that have been lemmatized using NLTK's WordNetLemmatizer on the words in the list `review_clean_original`.\n",
    "    \n",
    "**Note: This can take more than 10minutes to run on a laptop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Porter stemming on the results in review_clean_original\n",
    "\n",
    "review_clean_ps = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    ps_stems = []\n",
    "    for w in review_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        ps_stems.append(ps.stem(w))\n",
    "    \n",
    "    review_clean_ps.append(' '.join(ps_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Lemmatizer\n",
    "\n",
    "review_clean_wnl = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    \n",
    "    wnl_stems = []\n",
    "    token_tag = pos_tag(review_clean_original[i].split())\n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "\n",
    "    review_clean_wnl.append(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec4'></div>\n",
    "# Part 4: Sentiment Classification w scikit-learn, Feature vectors & Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: Explanation\n",
    "\n",
    "Derived from source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "We will now use scikit-learn to create numeric representations of the words in the reviews, using a method called Bag of Words. You can see this as learning a vocabulary from all the reviews and counting how many times a word appears in the reviews. For example, if we have two sentences:\n",
    "\n",
    "**Sentence 1:** \"cool students study cool data science\"\n",
    "\n",
    "**Sentence 2:** \"to know data science study data science\"\n",
    "\n",
    "The vocabulary of these two sentences can be summarized in a dictionary:\n",
    "\n",
    "    { cool, students, study, data, science, to, know }\n",
    "\n",
    "The bags of words count the number of times each word occur in a sentence. In Sentence 1, \"cool\" appears twice, and \"students\", \"study\", \"data\", and \"science\" appear once. The feature vector for Sentence 1 is:\n",
    "\n",
    "    Sentence 1: { 2, 1, 1, 1, 1, 0, 0 }\n",
    "\n",
    "    Sentence 2: { 0, 0, 1, 2, 2, 1, 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `fit` method in scikit-learns `CountVectorizer` creates the vocabulary of the bag-of-words model and fits the function to the data set you pass in.\n",
    "\n",
    "The`transform` method in `CountVectorizer` transforms text input to a feature matrix, where the rows are the document inputs and the columns are the words / feature in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example code BoW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"cool students study cool data science\"\n",
    "sent2 = \"to know data science study data science\"\n",
    "\n",
    "vect = CountVectorizer() #instantiate\n",
    "\n",
    "sents = np.array([sent1,sent2])\n",
    "\n",
    "vect.fit(sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of words in the vocabulary (and position in feature matrix):\\n')\n",
    "print(vect.vocabulary_)\n",
    "\n",
    "# vocabulary for the BoW model is stored in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to get feature vectors\n",
    "\n",
    "bag = vect.transform(sents)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "# the rows corresponds to the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names() # stored in the right places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in a DataFrame for interpretability\n",
    "\n",
    "pd.DataFrame(bag.toarray(), columns=vect.get_feature_names(), index=[sent1,sent2])\n",
    "\n",
    "# the number in the DataFrame is called Raw Term frequency raw term frequencies: \n",
    "# tf (t,d)—the number of times a term t occurs in a document d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying this strategy to the IMDB movie reviews\n",
    "\n",
    "The movie review data contains a lot of words. To limit the analysis we use the 5000 most frequent words from the cleaned reviews. To extract the bag of words features we will use scitkit-learn.\n",
    "\n",
    "The training data will be created by the `CountVectorizer` function from scikit-learn, and the training array will have 25000 rows (one for each review) and 5000 features (one for each vocabulary word).\n",
    "\n",
    "CountVectorizer can automatically handle text cleaning, but here we specify `None`, instead we did a step-by-step cleaning of the data in the earlier problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for review sentiment classification\n",
    "\n",
    "First split up the data set so that 80% are used as training samples (the first 20000 reviews and their sentiment) and 20% are used as validation samples (the last 5000 reviews and their sentiment). Use Random Forest to do numeric training on the features for the training samples from the Bag of Words and their respective sentiment labels for each review / feature vector. The number of trees is set to 50 as a default value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    review_clean_original, train['sentiment'], random_state=0, test_size=.2)\n",
    "\n",
    "\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transform the text data to feature\n",
    "# Only fit training data (to mimic real world)\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that it worked, \n",
    "# now we have fitted a model that can transform features\n",
    "# to sparse matrix representation\n",
    "\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_bag.toarray().shape) # 20,000 reviews, 2,000 feartures. just as expected\n",
    "print(test_bag.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(train_bag) # sparse matrix representation\n",
    "\n",
    "print(train_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasify with Random Forest model\n",
    "\n",
    "* Fit a Random Forest model to our bagged data set in order to do the sentiment analysis on `review_clean_original` and print the **validation accuracy** by using `forest.predict(test_bag)` and then comparing the resulting sentiment predictions with the ones stored in `y_test`.\n",
    "\n",
    "*This can take 2-3 mins to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Initialize a Random Forest classifier with 50 trees\n",
    "# hyperparameter n_estimators always set in instantiation\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the target variable\n",
    "\n",
    "forest = forest.fit(train_bag, y_train) # can take 2 mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions) # 100% training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test,valid_predictions) # 83% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# Is the number of False Positives and True negatives approx 50/50?\n",
    "metrics.confusion_matrix(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_predictions==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are the characteristics of False Positives for example?\n",
    "# Good practice when doing analysis\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test[(y_test.values==0) & (valid_predictions==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "# returns relative importance of all features.\n",
    "# they are in the order of the columns\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort importance scores\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "top_10 = indices[:10]\n",
    "\n",
    "# Get top ten features\n",
    "print([vec.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# put everything together in a function\n",
    "\n",
    "def predict_sentiment(cleaned_reviews, y=train[\"sentiment\"]):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print()\n",
    "    #Extract feature importnace\n",
    "    print('TOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vec.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Original cleaned to lemmatized and stemmed data set\n",
    "\n",
    "Now carry out the same analysis as above but on the `review_clean_ps` and `review_clean_wnl`. \n",
    "\n",
    "What data preprocessing strategy worked the best? Why do you think that is? (Feel free to change the number of features extracted in the bag of words model and the number of trees in the random forest model (i.e. the hyperparameters in our model), to see how it effects your accuracy. Is the accuracy better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Original Reviews')\n",
    "forest1,vec1 = predict_sentiment(review_clean_original)\n",
    "print('Porter Stemmer')\n",
    "forest2,vec2 = predict_sentiment(review_clean_ps)\n",
    "print('Lemmatizing')\n",
    "forest3,vec3 = predict_sentiment(review_clean_wnl)\n",
    "\n",
    "\n",
    "# It  seems like Porter Stemmer and Lemmatizing does not effect the results as much as we thought\n",
    "# This is just what Sebastian Raschka points out in his book Python Machine Learning:\n",
    "\n",
    "'''\n",
    "The Porter stemming algorithm is probably the oldest and simplest\n",
    "stemming algorithm. Other popular stemming algorithms include the\n",
    "newer Snowball stemmer (Porter2 or \"English\" stemmer) or the Lancaster\n",
    "stemmer (Paice-Husk stemmer), which is faster but also more aggressive\n",
    "than the Porter stemmer. Those alternative stemming algorithms are also\n",
    "available through the NLTK package (http://www.nltk.org/api/\n",
    "nltk.stem.html).\n",
    "\n",
    "While stemming can create non-real words, such as thu, (from thus) as\n",
    "shown in the previous example, a technique called lemmatization aims to\n",
    "obtain the canonical (grammatically correct) forms of individual words—\n",
    "the so-called lemmas. However, lemmatization is computationally more\n",
    "diffcult and expensive compared to stemming and, in practice, it has\n",
    "been observed that stemming and lemmatization have little impact on the\n",
    "performance of text classifcation (Michal Toman, Roman Tesar, and Karel\n",
    "Jezek. Infuence of word normalization on text classifcation. Proceedings of\n",
    "InSciT, pages 354–358, 2006).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec5'></div>\n",
    "## Appendix Kaggle submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to submit to Kaggle\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_cleaner( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"BoW_results.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to improve accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (worth 1p)\n",
    "\n",
    "* **Question:** Preprocess the reviews in any way you find suitable and build your own ML model that can predict the sentiment of movie reviews. Credit will be given if you can obtain a prediction accuracy of over 90%, when predicting the sentiments of the validation set (the last 5000 reviews). Train your model on the first 20000 reviews (with their sentiment as the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## See other solution notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
